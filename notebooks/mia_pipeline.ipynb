{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffff11f",
   "metadata": {},
   "source": [
    "# Membership Inference Attacks on Fine-Tuned LLMs\n",
    "\n",
    "**Phases 3–8: Tokenization → Training → MIA → Evaluation → Visualization → Sanity Checks**\n",
    "\n",
    "This notebook is designed to run on **Google Colab with a GPU runtime**.\n",
    "\n",
    "**Pre-requisites:** Upload `data/train.jsonl`, `data/val.jsonl`, and `data/nonmember.jsonl` (generated locally by `src/generate_dataset.py`).\n",
    "\n",
    "**Research Questions:**\n",
    "- **RQ1:** Does overfitting increase membership inference success?\n",
    "- **RQ2:** Are rare training samples more vulnerable to membership inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07985de3",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1 — Environment Setup (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 — Install dependencies\n",
    "!pip install -q torch transformers datasets accelerate scikit-learn matplotlib numpy pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 — Verify GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available:  {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device:      {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory:      {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device:    {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 — Fix all random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "print(f\"✓ All seeds fixed to {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b20e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 — Upload data files from local machine\n",
    "# Run this cell in Colab to upload train.jsonl, val.jsonl, nonmember.jsonl\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"models/regularized\", exist_ok=True)\n",
    "os.makedirs(\"models/overfitted\", exist_ok=True)\n",
    "os.makedirs(\"results/plots\", exist_ok=True)\n",
    "\n",
    "# --- Option A: Google Colab upload ---\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"Upload train.jsonl:\")\n",
    "    uploaded = files.upload()\n",
    "    for name, content in uploaded.items():\n",
    "        with open(f\"data/{name}\", \"wb\") as f:\n",
    "            f.write(content)\n",
    "    print(\"Upload val.jsonl:\")\n",
    "    uploaded = files.upload()\n",
    "    for name, content in uploaded.items():\n",
    "        with open(f\"data/{name}\", \"wb\") as f:\n",
    "            f.write(content)\n",
    "    print(\"Upload nonmember.jsonl:\")\n",
    "    uploaded = files.upload()\n",
    "    for name, content in uploaded.items():\n",
    "        with open(f\"data/{name}\", \"wb\") as f:\n",
    "            f.write(content)\n",
    "except ImportError:\n",
    "    print(\"Not running on Colab — assuming data/ files already exist.\")\n",
    "\n",
    "# Verify\n",
    "for f in [\"data/train.jsonl\", \"data/val.jsonl\", \"data/nonmember.jsonl\"]:\n",
    "    assert os.path.exists(f), f\"Missing: {f}\"\n",
    "    n = sum(1 for _ in open(f))\n",
    "    print(f\"  ✓ {f}: {n:,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d42b8",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3 — Data Processing & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdebbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 — Load and tokenize datasets\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "MODEL_NAME = \"distilgpt2\"\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer: {MODEL_NAME}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"pad_token: {tokenizer.pad_token!r} (id={tokenizer.pad_token_id})\")\n",
    "print(f\"max_length: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 — Tokenize train + val splits\n",
    "def tokenize_fn(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "raw = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"data/train.jsonl\",\n",
    "    \"val\": \"data/val.jsonl\",\n",
    "})\n",
    "\n",
    "tokenized = raw.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"id\", \"text\", \"group\"],\n",
    "    desc=\"Tokenizing\",\n",
    ")\n",
    "tokenized.set_format(\"torch\")\n",
    "\n",
    "train_dataset = tokenized[\"train\"]\n",
    "val_dataset = tokenized[\"val\"]\n",
    "\n",
    "print(f\"\\n--- Tokenization Summary ---\")\n",
    "print(f\"  Train:   {len(train_dataset):,} samples\")\n",
    "print(f\"  Val:     {len(val_dataset):,} samples\")\n",
    "print(f\"  Columns: {train_dataset.column_names}\")\n",
    "\n",
    "# Sanity check: decode one sample\n",
    "sample = train_dataset[0]\n",
    "decoded = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\n",
    "print(f\"\\n--- Sample (decoded) ---\")\n",
    "print(f\"  {decoded[:200]}...\")\n",
    "assert (sample[\"input_ids\"] == sample[\"labels\"]).all(), \"labels != input_ids\"\n",
    "print(\"  ✓ labels == input_ids confirmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c491ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 — Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "print(\"✓ Data collator ready (causal LM, mlm=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf591f59",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 4 — Model Training\n",
    "\n",
    "Train two models with **identical architecture, data, and optimizer** — only changing overfitting level:\n",
    "\n",
    "| Setting | Regularized | Overfitted |\n",
    "|---------|-------------|------------|\n",
    "| Epochs | 2 | 10 |\n",
    "| Weight Decay | 0.01 | 0.0 |\n",
    "| Early Stopping | Yes (patience=2) | No |\n",
    "| Learning Rate | 5e-5 | 5e-5 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e0b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 — Shared training function\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    output_dir,\n",
    "    epochs,\n",
    "    weight_decay,\n",
    "    learning_rate=5e-5,\n",
    "    use_early_stopping=False,\n",
    "    early_stopping_patience=2,\n",
    "    batch_size=8,\n",
    "):\n",
    "    \"\"\"Train distilgpt2 with the given hyperparameters.\"\"\"\n",
    "    set_seed(SEED)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=16,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_steps=100,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        load_best_model_at_end=use_early_stopping,\n",
    "        metric_for_best_model=\"eval_loss\" if use_early_stopping else None,\n",
    "        greater_is_better=False if use_early_stopping else None,\n",
    "        seed=SEED,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    callbacks = []\n",
    "    if use_early_stopping:\n",
    "        callbacks.append(EarlyStoppingCallback(\n",
    "            early_stopping_patience=early_stopping_patience\n",
    "        ))\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    train_result = trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Extract per-epoch logs\n",
    "    logs = []\n",
    "    for entry in trainer.state.log_history:\n",
    "        if \"eval_loss\" in entry:\n",
    "            logs.append({\n",
    "                \"epoch\": entry.get(\"epoch\"),\n",
    "                \"eval_loss\": entry[\"eval_loss\"],\n",
    "            })\n",
    "        elif \"loss\" in entry:\n",
    "            logs.append({\n",
    "                \"epoch\": entry.get(\"epoch\"),\n",
    "                \"train_loss\": entry[\"loss\"],\n",
    "            })\n",
    "\n",
    "    print(f\"\\n✓ Model saved to {output_dir}\")\n",
    "    return trainer, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 — Train Model A: Regularized\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MODEL A — REGULARIZED\")\n",
    "print(\"  epochs=2, weight_decay=0.01, early_stopping=True\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trainer_reg, logs_reg = train_model(\n",
    "    output_dir=\"models/regularized\",\n",
    "    epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=5e-5,\n",
    "    use_early_stopping=True,\n",
    "    early_stopping_patience=2,\n",
    ")\n",
    "\n",
    "print(\"\\nRegularized training logs:\")\n",
    "for entry in logs_reg:\n",
    "    print(f\"  {entry}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc53aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 — Train Model B: Overfitted\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MODEL B — OVERFITTED\")\n",
    "print(\"  epochs=10, weight_decay=0.0, early_stopping=False\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trainer_ovf, logs_ovf = train_model(\n",
    "    output_dir=\"models/overfitted\",\n",
    "    epochs=10,\n",
    "    weight_decay=0.0,\n",
    "    learning_rate=5e-5,\n",
    "    use_early_stopping=False,\n",
    ")\n",
    "\n",
    "print(\"\\nOverfitted training logs:\")\n",
    "for entry in logs_ovf:\n",
    "    print(f\"  {entry}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6109b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 — Save training logs and compute overfitting gaps\n",
    "\n",
    "def extract_epoch_metrics(logs):\n",
    "    \"\"\"Merge train_loss and eval_loss entries by epoch.\"\"\"\n",
    "    epochs = {}\n",
    "    for entry in logs:\n",
    "        e = entry.get(\"epoch\")\n",
    "        if e is None:\n",
    "            continue\n",
    "        e = round(e)\n",
    "        if e not in epochs:\n",
    "            epochs[e] = {}\n",
    "        if \"train_loss\" in entry:\n",
    "            epochs[e][\"train_nll\"] = entry[\"train_loss\"]\n",
    "        if \"eval_loss\" in entry:\n",
    "            epochs[e][\"val_nll\"] = entry[\"eval_loss\"]\n",
    "    result = []\n",
    "    for e in sorted(epochs.keys()):\n",
    "        row = {\"epoch\": e}\n",
    "        row.update(epochs[e])\n",
    "        result.append(row)\n",
    "    return result\n",
    "\n",
    "reg_metrics = extract_epoch_metrics(logs_reg)\n",
    "ovf_metrics = extract_epoch_metrics(logs_ovf)\n",
    "\n",
    "training_logs = {\n",
    "    \"regularized\": reg_metrics,\n",
    "    \"overfitted\": ovf_metrics,\n",
    "}\n",
    "\n",
    "# Compute final overfitting gaps\n",
    "for name, metrics in training_logs.items():\n",
    "    last = metrics[-1]\n",
    "    gap = last.get(\"val_nll\", 0) - last.get(\"train_nll\", 0)\n",
    "    print(f\"{name:15s} — final train_nll: {last.get('train_nll', '?'):.4f}, \"\n",
    "          f\"val_nll: {last.get('val_nll', '?'):.4f}, gap: {gap:.4f}\")\n",
    "\n",
    "with open(\"results/training_logs.json\", \"w\") as f:\n",
    "    json.dump(training_logs, f, indent=2)\n",
    "print(\"\\n✓ Saved results/training_logs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6e9509",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 5 — Membership Inference Attack\n",
    "\n",
    "**Threat model:** Black-box — attacker computes per-sample NLL only.\n",
    "\n",
    "**Signal:** Lower NLL → model is more confident → more likely a training member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab782fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 — NLL computation function\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def compute_nll(model, tokenizer, text, device):\n",
    "    \"\"\"Compute mean per-token NLL for a single text.\"\"\"\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        loss = model(**enc, labels=enc[\"input_ids\"]).loss\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def load_raw_jsonl(path):\n",
    "    \"\"\"Load JSONL to list of dicts.\"\"\"\n",
    "    records = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            records.append(json.loads(line))\n",
    "    return records\n",
    "\n",
    "\n",
    "print(\"✓ NLL functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 — Build balanced evaluation set\n",
    "#   Common: 1000 members + 1000 non-members\n",
    "#   Rare:   200 members + 200 non-members\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "train_records = load_raw_jsonl(\"data/train.jsonl\")\n",
    "nonmember_records = load_raw_jsonl(\"data/nonmember.jsonl\")\n",
    "\n",
    "# Split by group\n",
    "train_common = [r for r in train_records if r[\"group\"] == \"common\"]\n",
    "train_rare = [r for r in train_records if r[\"group\"] == \"rare\"]\n",
    "nm_common = [r for r in nonmember_records if r[\"group\"] == \"common\"]\n",
    "nm_rare = [r for r in nonmember_records if r[\"group\"] == \"rare\"]\n",
    "\n",
    "print(f\"Available — train common: {len(train_common)}, train rare: {len(train_rare)}\")\n",
    "print(f\"Available — nm common:    {len(nm_common)}, nm rare:    {len(nm_rare)}\")\n",
    "\n",
    "# Sample balanced subsets\n",
    "rng = random.Random(SEED)\n",
    "eval_members_common = rng.sample(train_common, min(1000, len(train_common)))\n",
    "eval_members_rare = rng.sample(train_rare, min(200, len(train_rare)))\n",
    "eval_nonmembers_common = rng.sample(nm_common, min(1000, len(nm_common)))\n",
    "eval_nonmembers_rare = rng.sample(nm_rare, min(200, len(nm_rare)))\n",
    "\n",
    "# Tag membership\n",
    "for r in eval_members_common + eval_members_rare:\n",
    "    r[\"is_member\"] = 1\n",
    "for r in eval_nonmembers_common + eval_nonmembers_rare:\n",
    "    r[\"is_member\"] = 0\n",
    "\n",
    "eval_pool = eval_members_common + eval_members_rare + eval_nonmembers_common + eval_nonmembers_rare\n",
    "rng.shuffle(eval_pool)\n",
    "\n",
    "n_mem = sum(1 for r in eval_pool if r[\"is_member\"] == 1)\n",
    "n_nm = sum(1 for r in eval_pool if r[\"is_member\"] == 0)\n",
    "n_rare = sum(1 for r in eval_pool if r[\"group\"] == \"rare\")\n",
    "print(f\"\\nEvaluation pool: {len(eval_pool)} total ({n_mem} members, {n_nm} non-members, {n_rare} rare)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c41df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 — Run MIA on both models\n",
    "import csv\n",
    "\n",
    "\n",
    "def run_mia(model_dir, eval_pool, output_csv, device):\n",
    "    \"\"\"Compute NLL for every record in eval_pool and save to CSV.\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_dir).to(device)\n",
    "    model.eval()\n",
    "    tok = AutoTokenizer.from_pretrained(model_dir)\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "    results = []\n",
    "    for record in tqdm(eval_pool, desc=f\"MIA [{model_dir}]\"):\n",
    "        nll = compute_nll(model, tok, record[\"text\"], device)\n",
    "        results.append({\n",
    "            \"id\": record[\"id\"],\n",
    "            \"nll\": nll,\n",
    "            \"is_member\": record[\"is_member\"],\n",
    "            \"group\": record[\"group\"],\n",
    "        })\n",
    "\n",
    "    with open(output_csv, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"id\", \"nll\", \"is_member\", \"group\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(f\"✓ Saved {len(results)} scores → {output_csv}\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Running MIA on regularized model...\")\n",
    "results_reg = run_mia(\"models/regularized\", eval_pool, \"results/regularized_scores.csv\", DEVICE)\n",
    "\n",
    "print(\"\\nRunning MIA on overfitted model...\")\n",
    "results_ovf = run_mia(\"models/overfitted\", eval_pool, \"results/overfitted_scores.csv\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db536fde",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 6 — Evaluation & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ea903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 — Compute AUC-ROC and attack advantage\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "def evaluate_mia(csv_path, label=None):\n",
    "    \"\"\"Compute MIA metrics overall and per subgroup.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    def _metrics(subset, name):\n",
    "        if len(subset) == 0 or subset[\"is_member\"].nunique() < 2:\n",
    "            return {\"subset\": name, \"n\": len(subset), \"auc\": None, \"advantage\": None, \"nll_gap\": None}\n",
    "        scores = -subset[\"nll\"].values  # lower NLL = more likely member\n",
    "        labels = subset[\"is_member\"].values\n",
    "        auc = roc_auc_score(labels, scores)\n",
    "        member_nll = subset[subset[\"is_member\"] == 1][\"nll\"].mean()\n",
    "        nonmember_nll = subset[subset[\"is_member\"] == 0][\"nll\"].mean()\n",
    "        return {\n",
    "            \"subset\": name,\n",
    "            \"n\": len(subset),\n",
    "            \"auc\": round(auc, 4),\n",
    "            \"advantage\": round(auc - 0.5, 4),\n",
    "            \"nll_gap\": round(nonmember_nll - member_nll, 4),\n",
    "            \"member_nll_mean\": round(member_nll, 4),\n",
    "            \"nonmember_nll_mean\": round(nonmember_nll, 4),\n",
    "        }\n",
    "\n",
    "    results = [\n",
    "        _metrics(df, \"all\"),\n",
    "        _metrics(df[df[\"group\"] == \"common\"], \"common\"),\n",
    "        _metrics(df[df[\"group\"] == \"rare\"], \"rare\"),\n",
    "    ]\n",
    "\n",
    "    if label:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"  {label}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    for r in results:\n",
    "        print(f\"  [{r['subset']:>6s}] n={r['n']:>5d}  AUC={r['auc']}  \"\n",
    "              f\"Advantage={r['advantage']}  NLL_gap={r['nll_gap']}\")\n",
    "\n",
    "    return results, df\n",
    "\n",
    "\n",
    "metrics_reg, df_reg = evaluate_mia(\"results/regularized_scores.csv\", \"REGULARIZED MODEL\")\n",
    "metrics_ovf, df_ovf = evaluate_mia(\"results/overfitted_scores.csv\", \"OVERFITTED MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1c4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 — Save metrics summary\n",
    "metrics_summary = {\n",
    "    \"regularized\": metrics_reg,\n",
    "    \"overfitted\": metrics_ovf,\n",
    "}\n",
    "\n",
    "with open(\"results/metrics_summary.json\", \"w\") as f:\n",
    "    json.dump(metrics_summary, f, indent=2)\n",
    "\n",
    "print(\"✓ Saved results/metrics_summary.json\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RQ1: Overfitting → MIA Success?\")\n",
    "print(\"=\" * 60)\n",
    "auc_reg_all = metrics_reg[0][\"auc\"]\n",
    "auc_ovf_all = metrics_ovf[0][\"auc\"]\n",
    "print(f\"  Regularized AUC (all): {auc_reg_all}\")\n",
    "print(f\"  Overfitted  AUC (all): {auc_ovf_all}\")\n",
    "print(f\"  Δ AUC: {round(auc_ovf_all - auc_reg_all, 4) if auc_reg_all and auc_ovf_all else 'N/A'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RQ2: Rare Samples More Vulnerable?\")\n",
    "print(\"=\" * 60)\n",
    "for name, metrics in [(\"Regularized\", metrics_reg), (\"Overfitted\", metrics_ovf)]:\n",
    "    auc_common = metrics[1][\"auc\"]\n",
    "    auc_rare = metrics[2][\"auc\"]\n",
    "    print(f\"  {name:15s} — Common AUC: {auc_common}, Rare AUC: {auc_rare}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3410820",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 7 — Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bb0680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 — Plot 1: ROC Curves (regularized vs overfitted)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
    "\n",
    "for df, label, color in [\n",
    "    (df_reg, f\"Regularized (AUC={metrics_reg[0]['auc']})\", \"tab:blue\"),\n",
    "    (df_ovf, f\"Overfitted (AUC={metrics_ovf[0]['auc']})\", \"tab:red\"),\n",
    "]:\n",
    "    scores = -df[\"nll\"].values\n",
    "    labels = df[\"is_member\"].values\n",
    "    fpr, tpr, _ = roc_curve(labels, scores)\n",
    "    ax.plot(fpr, tpr, label=label, color=color, linewidth=2)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], \"k--\", alpha=0.4, label=\"Random (AUC=0.5)\")\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_title(\"Membership Inference — ROC Curve\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"results/plots/roc_curves.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"✓ Saved results/plots/roc_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86730c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 — Plot 2: AUC Bar Chart (All / Common / Rare × model)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "subsets = [\"all\", \"common\", \"rare\"]\n",
    "x = np.arange(len(subsets))\n",
    "width = 0.35\n",
    "\n",
    "aucs_reg = [m[\"auc\"] or 0 for m in metrics_reg]\n",
    "aucs_ovf = [m[\"auc\"] or 0 for m in metrics_ovf]\n",
    "\n",
    "bars1 = ax.bar(x - width / 2, aucs_reg, width, label=\"Regularized\", color=\"tab:blue\", edgecolor=\"black\")\n",
    "bars2 = ax.bar(x + width / 2, aucs_ovf, width, label=\"Overfitted\", color=\"tab:red\", edgecolor=\"black\")\n",
    "\n",
    "ax.axhline(y=0.5, color=\"gray\", linestyle=\"--\", linewidth=1, label=\"Random\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([s.capitalize() for s in subsets])\n",
    "ax.set_ylabel(\"AUC-ROC\")\n",
    "ax.set_title(\"MIA Success by Model and Subgroup\")\n",
    "ax.set_ylim(0.4, 0.85)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, h + 0.005, f\"{h:.3f}\",\n",
    "                ha=\"center\", va=\"bottom\", fontsize=9, fontweight=\"bold\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"results/plots/auc_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"✓ Saved results/plots/auc_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 — Plot 3: Training Curves (train NLL vs val NLL per epoch)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "for ax, (name, metrics, color) in zip(axes, [\n",
    "    (\"Regularized\", reg_metrics, \"tab:blue\"),\n",
    "    (\"Overfitted\", ovf_metrics, \"tab:red\"),\n",
    "]):\n",
    "    epochs_list = [m[\"epoch\"] for m in metrics]\n",
    "    train_nlls = [m.get(\"train_nll\") for m in metrics]\n",
    "    val_nlls = [m.get(\"val_nll\") for m in metrics]\n",
    "\n",
    "    if any(v is not None for v in train_nlls):\n",
    "        valid = [(e, v) for e, v in zip(epochs_list, train_nlls) if v is not None]\n",
    "        ax.plot([e for e, _ in valid], [v for _, v in valid],\n",
    "                \"o-\", color=color, label=\"Train NLL\")\n",
    "    if any(v is not None for v in val_nlls):\n",
    "        valid = [(e, v) for e, v in zip(epochs_list, val_nlls) if v is not None]\n",
    "        ax.plot([e for e, _ in valid], [v for _, v in valid],\n",
    "                \"s--\", color=\"tab:orange\", label=\"Val NLL\")\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"NLL (Loss)\")\n",
    "    ax.set_title(f\"{name} Model\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"Overfitting Visualization — Train vs Val NLL\", fontsize=14)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"results/plots/training_curves.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"✓ Saved results/plots/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ce14c",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 8 — Validation & Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e70f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 — Sanity Check 1: Shuffled Labels Baseline\n",
    "# If the attack is real, shuffled labels should give AUC ≈ 0.5\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SANITY CHECK 1 — Shuffled Labels\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_trials = 100\n",
    "shuffled_aucs = []\n",
    "\n",
    "for _ in range(n_trials):\n",
    "    shuffled_labels = np.random.permutation(df_ovf[\"is_member\"].values)\n",
    "    auc_shuf = roc_auc_score(shuffled_labels, -df_ovf[\"nll\"].values)\n",
    "    shuffled_aucs.append(auc_shuf)\n",
    "\n",
    "mean_shuf = np.mean(shuffled_aucs)\n",
    "std_shuf = np.std(shuffled_aucs)\n",
    "print(f\"  Shuffled AUC ({n_trials} trials): {mean_shuf:.4f} ± {std_shuf:.4f}\")\n",
    "print(f\"  Expected: ~0.5\")\n",
    "if 0.45 < mean_shuf < 0.55:\n",
    "    print(\"  ✓ PASS — shuffled baseline is near random\")\n",
    "else:\n",
    "    print(\"  ✗ FAIL — shuffled baseline deviates from 0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b5920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 — Sanity Check 2: Pretrained (non-finetuned) Baseline\n",
    "# A model that was never fine-tuned should have no membership signal → AUC ≈ 0.5\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SANITY CHECK 2 — Pretrained Baseline (no fine-tuning)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "pretrained_model.eval()\n",
    "pretrained_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "pretrained_tokenizer.pad_token = pretrained_tokenizer.eos_token\n",
    "\n",
    "pretrained_nlls = []\n",
    "pretrained_labels = []\n",
    "pretrained_groups = []\n",
    "\n",
    "for record in tqdm(eval_pool, desc=\"Pretrained MIA\"):\n",
    "    nll = compute_nll(pretrained_model, pretrained_tokenizer, record[\"text\"], DEVICE)\n",
    "    pretrained_nlls.append(nll)\n",
    "    pretrained_labels.append(record[\"is_member\"])\n",
    "    pretrained_groups.append(record[\"group\"])\n",
    "\n",
    "auc_pretrained = roc_auc_score(pretrained_labels, [-n for n in pretrained_nlls])\n",
    "print(f\"  Pretrained AUC: {auc_pretrained:.4f}\")\n",
    "print(f\"  Expected: ~0.5\")\n",
    "if 0.45 < auc_pretrained < 0.55:\n",
    "    print(\"  ✓ PASS — pretrained model has no membership signal\")\n",
    "else:\n",
    "    print(\"  ⚠ WARN — pretrained AUC deviates from 0.5 (investigate template bias)\")\n",
    "\n",
    "del pretrained_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bda156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 — Sanity Check 3: NLL Distribution Histograms\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, (df, title) in zip(axes, [\n",
    "    (df_reg, \"Regularized\"),\n",
    "    (df_ovf, \"Overfitted\"),\n",
    "]):\n",
    "    members = df[df[\"is_member\"] == 1][\"nll\"]\n",
    "    nonmembers = df[df[\"is_member\"] == 0][\"nll\"]\n",
    "\n",
    "    ax.hist(members, bins=40, alpha=0.6, label=f\"Members (μ={members.mean():.3f})\", color=\"tab:blue\")\n",
    "    ax.hist(nonmembers, bins=40, alpha=0.6, label=f\"Non-members (μ={nonmembers.mean():.3f})\", color=\"tab:red\")\n",
    "    ax.axvline(members.mean(), color=\"tab:blue\", linestyle=\"--\", linewidth=2)\n",
    "    ax.axvline(nonmembers.mean(), color=\"tab:red\", linestyle=\"--\", linewidth=2)\n",
    "    ax.set_xlabel(\"NLL\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title(f\"{title} Model — NLL Distribution\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"results/plots/nll_distributions.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"✓ Saved results/plots/nll_distributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 — Save sanity check results\n",
    "\n",
    "sanity_results = {\n",
    "    \"shuffled_labels\": {\n",
    "        \"mean_auc\": round(float(mean_shuf), 4),\n",
    "        \"std_auc\": round(float(std_shuf), 4),\n",
    "        \"n_trials\": n_trials,\n",
    "        \"pass\": bool(0.45 < mean_shuf < 0.55),\n",
    "    },\n",
    "    \"pretrained_baseline\": {\n",
    "        \"auc\": round(float(auc_pretrained), 4),\n",
    "        \"pass\": bool(0.45 < auc_pretrained < 0.55),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Append to metrics summary\n",
    "with open(\"results/metrics_summary.json\", \"r\") as f:\n",
    "    full_metrics = json.load(f)\n",
    "full_metrics[\"sanity_checks\"] = sanity_results\n",
    "with open(\"results/metrics_summary.json\", \"w\") as f:\n",
    "    json.dump(full_metrics, f, indent=2)\n",
    "\n",
    "print(\"✓ Sanity check results appended to results/metrics_summary.json\")\n",
    "print(json.dumps(sanity_results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5 — Final Summary\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nResults files:\")\n",
    "for f in [\n",
    "    \"results/regularized_scores.csv\",\n",
    "    \"results/overfitted_scores.csv\",\n",
    "    \"results/metrics_summary.json\",\n",
    "    \"results/training_logs.json\",\n",
    "    \"results/plots/roc_curves.png\",\n",
    "    \"results/plots/auc_comparison.png\",\n",
    "    \"results/plots/training_curves.png\",\n",
    "    \"results/plots/nll_distributions.png\",\n",
    "]:\n",
    "    exists = \"✓\" if os.path.exists(f) else \"✗\"\n",
    "    print(f\"  {exists} {f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nRQ1 — Overfitting → MIA:\")\n",
    "print(f\"  Regularized AUC: {metrics_reg[0]['auc']}\")\n",
    "print(f\"  Overfitted  AUC: {metrics_ovf[0]['auc']}\")\n",
    "\n",
    "print(f\"\\nRQ2 — Rare vs Common:\")\n",
    "print(f\"  Overfitted Common AUC: {metrics_ovf[1]['auc']}\")\n",
    "print(f\"  Overfitted Rare   AUC: {metrics_ovf[2]['auc']}\")\n",
    "\n",
    "print(f\"\\nSanity Checks:\")\n",
    "print(f\"  Shuffled labels:    {'PASS' if sanity_results['shuffled_labels']['pass'] else 'FAIL'}\")\n",
    "print(f\"  Pretrained baseline: {'PASS' if sanity_results['pretrained_baseline']['pass'] else 'WARN'}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.6 — Download results (Colab only)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    import shutil\n",
    "\n",
    "    # Zip all results\n",
    "    shutil.make_archive(\"mia_results\", \"zip\", \".\", \"results\")\n",
    "    files.download(\"mia_results.zip\")\n",
    "    print(\"✓ Results downloaded as mia_results.zip\")\n",
    "except ImportError:\n",
    "    print(\"Not on Colab — results are in results/ directory.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
